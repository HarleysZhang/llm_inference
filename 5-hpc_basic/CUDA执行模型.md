- [一 CUDA 执行模型概述](#一-cuda-执行模型概述)
	- [1.1 英伟达 GPU 架构概述](#11-英伟达-gpu-架构概述)
	- [1.1.1 SM 和线程块](#111-sm-和线程块)
	- [1.1.2 什么是线程束（Warp）？](#112-什么是线程束warp)
	- [1.2，Volta 硬件架构](#12volta-硬件架构)
		- [1.2.1 Volta SM 硬件结构](#121-volta-sm-硬件结构)
	- [1.3 性能分析](#13-性能分析)
		- [1.3.1 事件和指标](#131-事件和指标)
- [二 理解线程束执行的本质](#二-理解线程束执行的本质)
	- [2.1 线程束和线程块](#21-线程束和线程块)
	- [2.2 线程束分化](#22-线程束分化)
	- [2.3 资源分配](#23-资源分配)
	- [2.4 指令、吞吐量、带宽](#24-指令吞吐量带宽)
	- [2.5 如何提高线程束占用率](#25-如何提高线程束占用率)
	- [2.6 用 nvprof 检测内存操作](#26-用-nvprof-检测内存操作)
- [三 如何提高并行性](#三-如何提高并行性)
	- [3.1 避免分支化](#31-避免分支化)
	- [3.2 展开循环](#32-展开循环)
	- [3.3 动态并行](#33-动态并行)
- [总结](#总结)

本文内容主要是以下几个小节:
- 通过配置文件驱动的方法优化内核
- 理解线程束执行的本质
- 增大 GPU 的并行性
- 掌握网格和线程块的启发式配置
- 学习多种 CUDA 的性能指标和事件
- 了解动态并行与嵌套执行

## 一 CUDA 执行模型概述

`CUDA` 编程模型中主要有两个抽象概念：**内存层次结构和线程层次结构**。

### 1.1 英伟达 GPU 架构概述

GPU的微观结构因不同厂商、不同架构都会有所差异，但核心部件、概念、以及运行机制大同小异，NVidia Tesla 微观架构如下图所示:

![Tesla 微观架构](../images/cuda_exec_model/tesla_architecture.webp)

SM 是 GPU 架构的核心，`GPU` 架构是围绕一个流式多处理器（SM）的可扩展阵列搭建的，可以通过复制这种架构的构建块来实现 `GPU` 的硬件并行，而 `SM` 的主要部件是：
- `CUDA` 计算核心
- 共享内存/一级缓冲
- 寄存器文件
- 加载/存储单元
- 特殊功能单元
- 线程束调度器

一个典型的 SM 结构图如下所示:

<img src="../images/cuda_exec_model/sm_structue.png" width="60%" alt="SM 结构">

### 1.1.1 SM 和线程块

GPU 中的每一个 SM 都能支持数百个线程并发执行，每个 GPU 通常有多个 SM，所以在一个GPU 上并发执行数千个线程是有可能的。当启动一个内核网格时，逻辑上的线程块实际是被分布在了可用的 `SM` 硬件上执行。
> 线程网格、线程块、线程是软件层面的逻辑概念，实际上在硬件上运行还是依靠 `SM`。

注意，多个线程块可能会被分配到同一个 `SM` 上，而且是根据 `SM` 资源的可用性进行调度的。但同一个线程块不可能分配到多个 `SM` 上。下图从逻辑视图和硬件视图的角度描述了CUDA 编程对应的组件：

![图3-2](../images/cuda_exec_model/图3-2.png)

寄存器和共享内存是 SM 中的稀缺资源。CUDA 将这些资源分配到 SM 中的所有常驻线程里。因此，这些有限的资源限制了在 SM 上活跃的线程束数量，活跃的线程束数量即对应于 SM 上的并行量。了解 SM 硬件组成的基本知识，有助于组织线程和配置内核执行以获得最佳的性能。

**CUDA 采用单指令多线程（`SIMT`）架构来管理和执行线程，每 `32` 个线程为一组，被称为线程束（`warp`）**。SIMD 和 SIMT 的区别：
- SIMD 所有执行单元必须在同一时刻执行完全相同的指令，而 SIMT 线程束内的线程可以独立执行不同的指令。
- SIMD 通过同步组来确保所有元素同步执行，SIMT 中，线程束内的线程虽然默认是同步执行的，但可以根据条件独立分支和执行不同指令。

### 1.1.2 什么是线程束（Warp）？

- 定义: 线程束是 CUDA 编程模型中的基本执行单位，由 32 个并行执行的线程组成。在 GPU 中，线程束是由流式多处理器（Streaming Multiprocessor, SM）调度和执行的。
- 同步性: 在一个线程束内的所有线程是同步执行的，这意味着它们在同一时刻执行相同的指令，但可以操作不同的数据。
- 调度和执行: GPU 中的调度器以线程束为单位进行调度。在每个时钟周期，SM 可以调度一个或多个线程束来执行指令。线程束中的所有线程共享同样的指令，但每个线程可以处理不同的数据元素（例如一个数组中的不同元素）。

### 1.2，Volta 硬件架构

与上一代 Pascal GP100 GPU 一样，GV100 GPU 由多个 GPU 处理集群 (GPC)、纹理处理集群 (TPC)、流式多处理器 (`SM`-STREAMING MULTIPROCESSOR) 和内存控制器组成。**完整的 GV100 GPU 包括**: 

1. `6` 个 `GPC`，每个 GPC 都有：
- `7` 个 `TPC`（每个包含两个 SM）
- `14` 个 `SM`

2. `84` 个 Volta `SM`，每个 `SM` 都有：
- 64 个 FP32 核心
- 64 个 INT32 核心
- 32 个 FP64 核心
- 8 个张量核心（Tensor Cores）
- 4 个纹理单元（texture units）

3. 8 个 512 位内存控制器（总共 4096 位）

**总结**： 一个完整的 `GV100 GPU` 含有 `84` 个 `SM`，总共有 `5376` 个 `FP32` 核心、`5376` 个 INT32 核心、2688 个 FP64 核心、672 个Tensor Cores 和 336 个纹理单元。 每个 HBM2 DRAM 堆栈都由一对内存控制器控制。 完整的 GV100 GPU 包括总计 6144 KB 的二级缓存。 

下图显示了具有 84 个 `SM` 的完整 GV100 GPU（注意，不同的产品可以使用不同配置的 GV100）。Tesla V100 加速器使用 80 个 SM。 

![具有 84 个 SM 单元的 Volta GV100 全 GPU](../images/nvidia_gpu/GV100_GPU_hardware_structure.png)
表 1 比较了过去五年的 NVIDIA Tesla GPU。

下表展示了 NVIDIA Tesla GPU 的比较。

![nvidia_tesla_gpu](../images/nvidia_gpu/nvidia_tesla_gpu.png)

#### 1.2.1 Volta SM 硬件结构

`GV100 GPU` 有 `84` 个 `SM`。与 Pascal GP100 类似，GV100 的每个 SM 都包含 `64` 个 FP32 内核和 `32` 个 FP64 内核。 但是，`GV100 SM` 使用了**一种新的分区方法**来提高 SM 利用率和整体性能。 

- `GP100` `SM` 被划分为**两个处理块**，每个处理块具有 `32` 个 FP32 内核、16 个 FP64 内核、一个指令缓冲区、一个 `warp` 调度程序、两个调度单元和一个 128 KB 的寄存器文件。 
- `GV100` `SM` 分为**四个处理块**，每个处理块有 `16` 个 FP32 内核、8 个 FP64 内核、16 个 INT32 内核、两个用于深度学习矩阵运算的新混合精度 Tensor 内核、一个新的 L0 指令缓存、一个 warp 调度程序、一个调度单元和一个 64 KB 的寄存器文件。新的 L0 指令缓存现在用于每个分区，用以提供比以前的 NVIDIA GPU 中使用的指令缓冲区更高的效率。 

总的来说，与前几代 GPU 相比，GV100 支持更多的**线程、线程束**和**线程块**。 共享内存和 L1 资源的合并使每个 Volta SM 的共享内存容量增加到 96 KB，而 GP100 为 64 KB。

Volta GV100 流式多处理器(SM)架构如下图像所示：

![Volta GV100 流式多处理器(SM)架构](../images/nvidia_gpu/Volta_sm.png)

### 1.3 性能分析

性能分析工具可以检测核函数中影响性能的瓶颈。CUDA 提供了两个主要的性能分析工具: `nvvp`，独立的可视化分析器；`nvprof`，命令行分析器。
- nvvp 可以可视化 CPU 与 GPU 上的程序活动的时间表，包括内核执行、内存传输和 CUDA 的 API 调用，也可以获得硬件计数器和 **CUDA 内核的性能指标**。
- nvprof 功能和 `nvvp` 差不多，不同的是它是命令行工具。

#### 1.3.1 事件和指标

在 CUDA 性能分析中，事件是可计算的活动，它对应**一个在内核执行期间被收集的硬件计数器**。指标是内核的特征，它由一个或多个事件计算得到。请记住以下概念事件和指标：
- 大多数计数器通过流式多处理器来报告，而不是通过整个GPU。
- 一个单一的运行只能获得几个计数器。有些计数器的获得是相互排斥的。多个性能分析运行往往需要获取所有相关的计数器。
- 由于 GPU 执行中的变化（如线程块和线程束调度指令），经重复运行，计数器值可能不是完全相同的。

分析性能的一个关键是如何使用不同的计数器和指标，从多个角度分析内核。另外，影响内核性能的因素有 3 种：
- **内存带宽**
- **计算资源（算力）**
- **指令和内存延迟**

总结：想要写出更好性能的内核函数，必须对 `gpu` 硬件架构和资源了解。在本文的后续部分，会描述**硬件的概念是如何与性能指标联系起来的，以及性能指标是如何被用于指导性能的**。

## 二 理解线程束执行的本质

在逻辑上，在内核中所有的线程都是并行地运行的；但在硬件上，不是所有线程在物理上都可以同时并行地执行。

### 2.1 线程束和线程块

线程网格、线程块、线程束、SM 是如何组织运转起来的呢？

**线程束是 SM 中的基本执行单元**。当一个线程块的网格被启动后，网格中的线程块分布在 SM 中。一旦线程块被调度到一个 SM 上，线程块中的线程会被进一步划分为线程束。一个线程束由32 个连续的线程组 成，在一个线程束中，所有的线程按照单指令多线程（SIMT）方式执行；也就是说，所有线程都执行相同的指令，每个线程在私有数据上进行操作。下图展示了线程块的逻辑视图和硬件视图之间的关系。

![逻辑和物理视图](../images/cuda_exec_model/thread_wrap.png)

从逻辑和物理硬件的角度看线程块和线程束的关系：

- **从逻辑角度来看，线程块是线程的集合，它们可以被组织为一维、二维或三维布局**。
- 从硬件角度来看，线程块是一维线程束的集合。在线程块中线程被组织成一维布局，每32个连续线程组成一个线程束

### 2.2 线程束分化

**在同一线程束中的线程执行不同的指令，被称为线程束分化**。线程束分化会导致性能明显地下降，其只会发生在同一个线程束中。

如果使用线程束方法（而不是线程方法）来交叉存取数据，可以避免线程束分化，并且设备的利用率可达到 100%。条件 `（tid/warpSize）%2==0` 使分支粒度是线程束大小的倍数；偶数编号的线程执行 if 子句，奇数编号的线程执行else子句。

```cpp
global_ void mathKernel2(void)
{
    int tid = blockIdx.x * blockDim.x + threadIdx.x;
    float a，b;
    a = b = 0.0f;
    if ((tid / warp_size) % 2 ==0){
        a = 100.0f;
    } else {
        b = 200.0f;
    }
    c[tid] = a + b;
}
```

### 2.3 资源分配

线程束的本地执行上下文主要由以下资源组成：
- 程序计数器
- 寄存器
- 共享内存

**对于一个给定的内核，同时存在于同一个 SM 中的线程块和线程束的数量取决于在 SM 中可用的且内核所需的寄存器和共享内存的数量**。

![线程束和寄存器数量的关系](../images/cuda_exec_model/sm_register.png)

### 2.4 指令、吞吐量、带宽

考虑到指令延迟，指令可以被分为两种基本类型：
- **算术指令**：其延迟 `latency` 是一个算术操作从开始到它产生输出之间的时间。算术操作为 10～20 个周期。
- **内存指令**：其延迟 `latency` 是指发送出的加载或存储操作和数据到达目的地之间的时间。全局内存访问为 400～800 个周期。

如何估算隐藏延迟所需要的活跃线程束的数量？利特尔法则（Little’s Law）可以提供一个合理的近似值。它起源于队列理论中的一个定理，它也可以应用于 GPU 中，形象地说明了利特尔法则。

所需线程束数量 ＝ 延迟 × 吞吐量

![利特尔法则](../images/cuda_exec_model/latency_throughput.png)

带宽通常是指理论峰值，而吞吐量是指已达到的值。**带宽通常是用来描述单位时间内最大可能的数据传输量，而吞吐量是用来描述单位时间内任何形式的信息或操作的执行速度**，例如，每个周期完成多少个指令。

GPU 的内存带宽是指 GPU 在单位时间内能够从其显存（VRAM）中读取或写入的数据量，也可以理解为 **SM 访问设备内存的速度**。GPU 内存带宽的计算公式为：

$$\text{内存带宽 (GB/s)} = \text{显存频率 (GHz)} \times \text{总线宽度 (bit)} \times \text{每次传输的字节数 (Bytes/transfer)} \times 2$$

其中：
- **显存频率**：显存芯片的工作频率，通常以 `GHz` 为单位。
- **总线宽度**：显存与GPU之间的数据总线宽度，通常以 `bit` 为单位，如256-bit、384-bit等。
- **每次传输的字节数**：这是通过显存芯片每个时钟周期能够传输的字节数。对于`GDDR` 显存，通常是双倍的数据速率（DDR），因此在公式中乘以2。

以 A100-40gb 显卡为例，其显存参数如下
- 显存类型: HBM2
- 显存总线宽度: 5120位（每个通道320位，共16个通道）
- 显存频率: 1215 MHz（1.215 GHz）
- 显存带宽: 1555 GB/s（官方提供）

$$\text{A100}\ 显存带宽 = 1.215 \times \frac{5120}{8}\times 2 = 1555.2\ \text{GB/s}$$

### 2.5 如何提高线程束占用率

在每个 CUDA 核心里指令是顺序执行的。当一个线程束阻塞时，SM 切换执行其他符合条件的线程束。理想情况下，我们想要有足够的线程束占用设备的核心。占用率是每个 SM 中活跃的线程束占最大线程束数量的比值。

$$线程束占用率 = \frac{活跃的线程束数量}{最大线程束数量}$$

注意，极端地操纵线程块会限制资源的利用：
- **小线程块**：每个块中线程太少，会在所有资源被充分利用之前导致硬件达到每个 SM 的线程束数量的限制。
- **大线程块**：每个块中有太多的线程，会导致在每个 SM 中每个线程可用的硬件资源较少。

下述是 GPU 核函数配置的一些总结，以尽可能提高 GPU 计算资源利用率。
- **保持每个块 `block` 中线程数量是线程束大小（32）的倍数**；
- 避免块太小：每个块至少要有 128 或 256 个线程
- 根据内核资源的需求调整块大小；
- 块的数量要远远多于 SM 的数量，从而在设备中可以显示有足够的并行；
- 通过实验得到最佳执行配置和资源使用情况。

但值得注意的是，充分的占用率不是性能优化的唯一目标。内核一旦达到一定级别的占用率，进一步增加占用率可能不会改进性能，更高的占用率也并不一定意味着有更高的性能，因为还有其他因素限制 GPU 的性能。

`cudaDeviceSynchronize` 函数可以用来阻塞主机应用程序，直到所有的 CUDA 操作（复制、核函数等）完成。

### 2.6 用 nvprof 检测内存操作

以二维矩阵求和函数为例:
```cpp
__global__ void sumMatrixOnGPu2D(float *A, float *B, float *C, int NX, int Ny){
    unsigned int ix = blockIdx.x*blockDim.x + threadIdx.x;
  	unsigned int iy = blockIdx.y * blockDim.y + threadIdx.y;
  	unsigned int idx = iy * NX + ix; // 内存索引 idx 的计算原理参考前文代码
  	if (ix < NX && iy < NY) 
    	C[idx]=A[idx]+ B[idx];
}
```

测试一组基础线程块的配置，尤其是大小为（32，32），（32，16），（16，32）和（16，16）的线程块。在 Tesla M2070 上输出以下结果：

![性能测试时间](../images/cuda_exec_model/nvprof0.png)

在 sumMatrix 内核函数中（C[idx]＝A[idx]＋B[idx]）中有 3 个内存操作：两个内存加载和一个内存存储。可以使用 nvprof 检测这些内存操作的效率。

首先，用 gld_throughput 指标检查**内核的内存的读取吞吐量**，从而得到每个执行配置的差异：

![吞吐量](../images/cuda_exec_model/nvprof1.png)

第四种情况中的加载吞吐量最高，第二种情况中的加载吞吐量大约是第四种情况的一半，但第四种情况却比第二种情况慢。所以，更高的加载吞吐量并不一定意味着更高的性能。

其次，用 gld_efficiency 指标检测**全局内存加载效率**，它表示每次全局内存加载请求中，实际被使用的数据字节与总加载数据字节的比值，计算公式如下：

$$\text{gld\_efficiency} = \frac {实际使用的全局内存加载字节数}{总全局内存加载字节数} \times 100\% $$

- 实际使用的全局内存加载字节数：核函数中实际被线程使用的字节数。
- 总全局内存加载字节数：核函数中所有线程加载的总字节数，包括未使用或无效的数据。

`gld_efficiency` 越高，说明内存访问越高效，优化方法是使用对齐访问、减少分支和提高内存访问的空间局部性。

![吞吐量](../images/cuda_exec_model/nvprof2.png)

最后两种情况下的加载效率是最前面两种情况的一半。这解释了为什么最后两种情况下更高的加载吞吐量和可实现占用率没有产生较好的性能，因为**加载的有效性（即效率）是较低的**。

最后两种情况的共同特征是它们在最内层维数中块的大小是线程束的一半。如前所述，**对网格和块启发式算法来说，最内层的维数（block.x）应该总是线程束大小的倍数**。

## 三 如何提高并行性

### 3.1 避免分支化
> 额外知识：**归约操作**（Reduction Operation）是一种将多个数据值组合成一个单一值的运算。在并行计算中，归约操作通常用于将分散在不同处理器或计算节点上的数据（如梯度）汇总起来，以便进行全局计算或同步。归约操作可以是简单的，如求和（Sum）、求平均（Mean）、求最大值（Max）或求最小值（Min），也可以是更复杂的自定义运算。

分支化指的是同一线程束（`warp`）内的线程因为**条件语句**（如 if-else、switch 等）执行不同路径，导致这些线程需要分别执行不同的指令，这会降低并行效率，导致内核性能变差。

以数组求和为例，成对的并行求和实现可以被分为以下两种类型:
- **相邻配对**: 元素与它们直接相邻的元素配对
- **交错配对**: 根据给定的跨度配对元素

相邻配对法的 `C++` 实现代码如下所示:

```cpp
#include <stdio.h>

// 相邻配对求和函数
int adjacent_pair_sum(int* arr, int n) {
    while (n > 1) {
        for (int i = 0; i < n / 2; i++) {
            arr[i] = arr[2 * i] + arr[2 * i + 1];
        }
        n /= 2; // 数组长度减半
    }
    return arr[0]; // 最终的和
}

int main() {
    int arr[] = {1, 2, 3, 4, 5, 6, 7, 8}; // 示例数组
    int n = sizeof(arr) / sizeof(arr[0]);
    
    int result = adjacent_pair_sum(arr, n);
    printf("Sum of array elements: %d\n", result);
    
    return 0;
}
```

### 3.2 展开循环

循环展开通过**将循环体中的代码“展开”成多个相似的操作，从而减少循环的执行次数和控制开销**。

如下两个数组求和代码：

```cpp
for (int i = 0; i<100;i++) {
	a[i] = b[i] + c[i]
}
```

重复操作一次循环体，迭代次数可减少为原来的一半:

```cpp
for (int i = 0; i<100;i+=2) {
	a[i] = b[i] + c[i];
	a[i + 1] = b[i+1] + c[i+1]
}
```

高级语言层面上来看，循环展开使性能提高的原因可能不是显而易见的，这种提升来其实是来自于编译器执行循环展开时低级指令的改进和优化。在 GPU 编程中，循环展开的目的是为了优化流水线处理和增加并发操作来提高计算性能。

### 3.3 动态并行

CUDA 的动态并行允许在 GPU 端直接创建和同步新的 GPU 内核。

在动态并行中，内核执行分为两种类型：父母和孩子。父线程、父线程块或父网格启动一个新的网格，即子网格。子线程、子线程块或子网格被父母启动。子网格必须在父线程、父线程块或父网格完成之前完成，只有在所有的子网格都完成之后，父母才会完成。下图说明了父网格和子网格的适用范围。

![父网格和子网格](../images/cuda_exec_model/father_gird.png)

在 GPU上 嵌套输出 Hello World 的实例代码:

```cpp
__global__ void nestedHelloWorld(int const iSize, int iDepth)
{
	int tid = threadIdx.x;
	printf("Recursion=%d: Hello World from thread %d"
			"block td\n", iDepth, tid, blockIdx.x);
	// condition to stop recursive execution
	if(iSize ==1) return;
	// reduce block size to half
	int nthreads = iSize>>1;
	// thread 0 launches child grid recursively
	if(tid ==0 && nthreads >0){
		nestedHelloWorld<<<1, nthreads>>>(nthreads, ++iDepth);
		printf("------> nested execution depth: %d\n" iDepth);
	}
```

每个线程的核函数执行，会先输出 “Hello World”。接着，每个线程检查自己是否该停止。如果在这个嵌套层里线程数大于1，线程0就递归地调用一个带有线程数一半的子网格。用以下命令编译代码:

```bash
nvcc -arch=sm35 -rdc=true nestedHelloWorld.cu -o nestedHelloWorld -lcudadevrt
```
因为动态并行是由设备运行时库所支持的，所以 nestedHelloWorld 函数必须在命令行使用 `-lcudadevrt` 进行明确链接。当 `-rdc` 标志为 true 时，它强制生成可重定位的设备代码，这是动态并行的一个要求。程序编译运行后输出如下所示:

![动态嵌套执行](../images/cuda_exec_model/dynamic_exec.png)

从输出信息中可见，由主机调用的父网格有1个线程块和8个线程。nestedHelloWorld 核函数递归地调用三次，每次调用的线程数是上一次的一半。

## 总结

在 GPU 设备上，CUDA 执行模型有两个最显著的特性：
- 使用 SIMT 方式在线程束中执行线程
- 在线程块与线程中分配了硬件资源

动态并行使设备能够直接创建新的工作。它确保我们可以用一种更自然和更易于理解的方式来表达递归或依赖数据并行的算法。为实现一个有效的嵌套内核，必须注意设备运行时的使用，其包括子网格启动策略、父子同步和嵌套层的深度。